\documentclass{article}
\usepackage{amsmath,amssymb,amsthm,enumitem} % Some standard math packages.
\usepackage{titling} % Enables \setlength{\droptitle}
\usepackage{parskip} % Cleaner paragraph display
\usepackage[margin=1in]{geometry} % Adjusts margins.
\usepackage[utf8]{inputenc} % USe UTF-8 input encoding instead of default ASCII.
\usepackage[]{forest} % Draws trees.
\usepackage{fancyvrb} % Allows Verbatim sections with line numbers and such. Note the capital V.
\usepackage{pgfplots}
\pgfplotsset{compat=1.6}
\newcommand {\todo}[1] {{\textbf{\color{red}#1}}}

\title{CS 584 Research Project}
\author{ Dylan Laufenberg }
\date{June 5, 2018}

\begin{document}
\maketitle

\paragraph{Project topic} I will implement a variety of data structures that maintain total orders on their data, e.g. 2-3 trees, treaps, and skip lists; I will choose at least one deterministic structure as a reference and at least one randomized data structure for comparison. I will experimentally evaluate the rates of growth of their run times for insertions, searches, and deletions. Based on these data, I will discuss how the performance I observe compares to the predicted asymptotic performance.

\newpage
\section{Introduction}
This paper aims to illuminate various facets of the relationship between asymptotic complexity and empirical performance. The vehicle for this exploration is a series of benchmarks of data structures that perform similar jobs in very different ways. These include \todo{LIST OF DATA STRUCTURES}. For many tasks, they have the same asymptotic complexity, e.g. average complexity of $O(\log n)$ for search, insert, and delete operations, with worst-case complexity of $O(n)$ for each. This paper examines the performance characteristics of each and investigates how comparable the performance characteristics of these asymptotically equivalent data structures really are.

\section{Testing Methodology}

The core of the analysis in this paper is a collection of data sets gathered through benchmarking. Thus, it is essential to establish a reliable testing methodology, enforced through automation, to gather accurate data and ensure the integrity of the analysis that follows. The benchmarking procedure is standardized and automated through benchmark.py.

Noise in the graphs is of paramount importance to the accuracy of the data sets. To smooth out small spikes in running times, each data point in each test consists of 1000 operations. At the start of the operation, the data structure contains the number of elements indicated on the graph. For instance, a data point for the insertion operation and labeled as having a data structure size of 100,000 elements consists of the average running time for the insertion operations on elements 100,001 through 101,000. This, of course, affects the overall result, as the final element is likely to require more work than the first element (in the case of insertion). In order to minimize these differences, the data structure sizes are no smaller than 100,000 elements, so that the greatest variation in data structure size from the start of any given data point's sample range to the end of the range is 1\%.

In order to compensate for larger variations in system load and running time, each benchmark as described above is run three times on the same input data, and for each data point, the median of the three runs is used.

\section{Data Structure Selection}
Many implementations exist for each of these data structures, since they are well-known. 

\newpage
\section{Testing TikZ Pictures}

\input{plots/sample.tex}
\end{document}
